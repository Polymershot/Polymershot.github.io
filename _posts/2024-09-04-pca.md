## Understanding Principal Component Analysis (The Big Picture & Proofs)

**Prerequisites**:
- Linear Algebra
- Calculus 3
- Up to Junior level Stat Courses (Probability/Stats Theory and Such)

This is mainly meant to strengthen the conceptial understanding of PCA (with some proofs that aren't too scary). Reading the two references alone should suffice for a much better understanding of PCA, but for me, I had to put it all together in a different way. Hope this can help someone who always struggled with why the eigenvalues were sorted in descending order and what maximizing variance really means (Sorry if this is too much already!).

---

### The Setup

##### ***Dimensionality Reduction***

When you first think of PCA, you probably think about dimensionality reduction right? We want to reduce the number of columns/dimensions while preserving as much information as possible. 
For example, it's like turning your data that resembles a sphere into something elliptical.

<p align="center">

  <img src="https://github.com/user-attachments/assets/1fb2e58a-c830-47ae-b7a4-65fec60afac0">

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/6aa956aa-4321-4889-8196-a2b499b63f7a">

</p>

#### ***Data Transformation(s)***
Now that we've estatblished what the general idea of dimensionality reduction, let's discuss what needs to be done with the data before we procced with PCA. The main thing to remember is that the data should be centered and scaled for "computation" purposes. Centering the data means each column in your data should have a mean of 0 and scaling the data means that each column in the data should be in terms of the same scale (units). To do this, z-score standardization is used. From this, each column will have $\mu$ = 0 and $\sigma$ = 1.

<p align="center">

  <img src="https://github.com/user-attachments/assets/90b37e0e-9de3-4242-9174-e114432bbed0">
  <strong><u>Reference #4 (pg.4)</u></strong> 

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/822e00c0-2ea5-4b97-a698-91a0495fcd86">


</p>

If this doesn't make sense or if you want to go more in-depth, please refer to Reference #4 (pg.6).


### Visualizing the Problem




### The Nitty Gritty




### Bringing it all Together









### References 
(Warning: Clicking the links will auto-download a pdf!)

1. Lagrange Method:
> https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-20-PCA-annotated.pdf

2. Proofs:
> https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf

3. Visualizing PCA
> https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

4. Visualizing PCA/Proof
> https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-16-PCA.pdf

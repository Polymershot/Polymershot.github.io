## Understanding Principal Component Analysis (The Big Picture & Proofs)

**Prerequisites**:
- Linear Algebra
- Calculus 3
- Up to Junior level Stat Courses (Probability/Stats Theory and Such)

This is mainly meant to strengthen the conceptial understanding of PCA (with some proofs that aren't too scary). Reading the two references alone should suffice for a much better understanding of PCA, but for me, I had to put it all together in a different way. Hope this can help someone who always struggled with why the eigenvalues were sorted in descending order and what maximizing variance really means (Sorry if this is too much already!).

---

### The Setup

##### ***Dimensionality Reduction***

When you first think of PCA, you probably think about dimensionality reduction right? We want to reduce the number of columns/dimensions while preserving as much information as possible. 
For example, it's like turning your data that resembles a sphere into something elliptical.
> ![image](https://github.com/user-attachments/assets/1fb2e58a-c830-47ae-b7a4-65fec60afac0)
> ![image](https://github.com/user-attachments/assets/6aa956aa-4321-4889-8196-a2b499b63f7a)

#### ***Data Transformation(s)***
Now that we've estatblished what the general idea of dimensionality reduction, let's discuss what needs to be done with the data before we procced with PCA.

### Visualizing the Problem




### The Nitty Gritty




### Bringing it all Together









### References 
(Warning: Clicking the links will auto-download a pdf!)

1. Lagrange Method:
> https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-20-PCA-annotated.pdf

2. Eigenvalues/Optimization Problem:
> https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf


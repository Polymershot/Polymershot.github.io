## Understanding Principal Component Analysis (The Big Picture & Proofs)

**Prerequisites**:
- Linear Algebra
- Calculus 3
- Up to Junior level Stat Courses (Probability/Stats Theory and Such)

This is mainly meant to strengthen the conceptial understanding of PCA (with some proofs that aren't too scary). If this post isn't too in-depth for you, feel free to just check out the references. I mainly want this guide to help other stats students like me who struggle. 

---

### The Setup

##### ***Dimensionality Reduction***

When you first think of PCA, you probably think about dimensionality reduction right? We want to reduce the number of columns/dimensions while preserving as much information as possible. 
For example, it's like turning your data that resembles a sphere into something elliptical.

<p align="center">

  <img src="https://github.com/user-attachments/assets/1fb2e58a-c830-47ae-b7a4-65fec60afac0">

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/6aa956aa-4321-4889-8196-a2b499b63f7a">

</p>

#### ***Data Transformation(s)***
Now that we've estatblished what the general idea of dimensionality reduction, let's discuss what needs to be done with the data before we procced with PCA. The main thing to remember is that the data should be centered and scaled for "computation" purposes. Centering the data means each column in your data should have a **mean of 0** and scaling the data means that each column in the data should be in terms of the **same scale (units)**. The mean being set to 0 should make sense later on hopefully and the scaling is due to the fact that some columns might be "favored" more due to their units. To apply this kind of transofrmation, **z-score standardization** is used. From this, each column will have $$\mu$$ = 0 and $$\sigma$$ = 1.

<p align="center">

  <img src="https://github.com/user-attachments/assets/90b37e0e-9de3-4242-9174-e114432bbed0">
  <strong><i>Reference #4 (pg.4)</i></strong> 

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/822e00c0-2ea5-4b97-a698-91a0495fcd86">


</p>

If this doesn't make sense or if you want to go more in-depth, please refer to Reference #4 (pg.6).


### Visualizing the Problem

Let's focus on the two-dimensional case so we can understand what the goal of PCA is. Take a look at the pictures below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/946841ff-b164-449e-8d70-e4ac6a9a9b6d">
</p>

<p align="center">
<strong><i>Reference #3</i></strong>
</p>

---

<p align="center">
  <img src="https://github.com/user-attachments/assets/8fcabfc4-9df8-4df5-bb6a-541bf4610a88">
</p>

<p align="center">
<strong><i>Reference #4 (pg.12) </i></strong>
</p>

Alright, I know that is a lot to take in but let's break it down. Remember how I said the "main" point of PCA was dimensionality reduction; the main way to achieve that is through the two methods (maximizing variance and minimizing residuals). However, before I talk about what those two things even mean, let's go back to the second picture. 


  
<p align="center">

$$
\begin{align}
\large{x_i:} &\text{Think of this as some data point} \\
\\
\large{\lvert x_i \rvert} \longrightarrow \large{\sqrt{x^2_1 + x^2_2...}} &\text{Basically, square the coordinates of the data points and add them all up; then take the square root at the end.}  \\
\\
\large{x'_i:} &\text{The projection of the datapoint onto the $\vv{\text{v_1}}$ vector} \\
\large{x_i - x'_i:} &\text{Think of this as like the **residuals**} \\
\end{align}
$$
</p>





### The Nitty Gritty




### Bringing it all Together









### References 
(Warning: Clicking the links will auto-download a pdf!)

1. Lagrange Method:
> https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-20-PCA-annotated.pdf

2. Proofs:
> https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf

3. Visualizing PCA
> https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

4. Visualizing PCA/Proof
> https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-16-PCA.pdf

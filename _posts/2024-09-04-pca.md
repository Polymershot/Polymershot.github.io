## Understanding Principal Component Analysis (The Big Picture & Proofs)

**Prerequisites**:
- Linear Algebra
- Calculus 3
- Up to Junior level Stat Courses (Probability/Stats Theory and Such)

This is mainly meant to strengthen the conceptial understanding of PCA (with some proofs that aren't too scary). If this post isn't too in-depth for you, feel free to just check out the references. I mainly want this guide to help other stats students like me who struggle. 

---

### The Setup

##### ***Dimensionality Reduction***

When you first think of PCA, you probably think about dimensionality reduction right? We want to reduce the number of columns/dimensions while preserving as much information as possible. 
For example, it's like turning your data that resembles a sphere into something elliptical.

<p align="center">

  <img src="https://github.com/user-attachments/assets/1fb2e58a-c830-47ae-b7a4-65fec60afac0">

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/6aa956aa-4321-4889-8196-a2b499b63f7a">

</p>

#### ***Data Transformation(s)***
Now that we've estatblished what the general idea of dimensionality reduction, let's discuss what needs to be done with the data before we procced with PCA. The main thing to remember is that the data should be centered and scaled for "computation" purposes. Centering the data means each column in your data should have a **mean of 0** and scaling the data means that each column in the data should be in terms of the **same scale (units)**. The mean being set to 0 should make sense later on hopefully and the scaling is due to the fact that some columns might be "favored" more due to their units. To apply this kind of transofrmation, **z-score standardization** is used. From this, each column will have $$\mu$$ = 0 and $$\sigma$$ = 1.

<p align="center">

  <img src="https://github.com/user-attachments/assets/90b37e0e-9de3-4242-9174-e114432bbed0">
  <strong><i>Reference #4 (pg.4)</i></strong> 

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/822e00c0-2ea5-4b97-a698-91a0495fcd86">


</p>

If this doesn't make sense or if you want to go more in-depth, please refer to Reference #4 (pg.6).


### Visualizing the Problem

Let's focus on the two-dimensional case so we can understand what the goal of PCA is. Take a look at the pictures below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/946841ff-b164-449e-8d70-e4ac6a9a9b6d">
</p>

<p align="center">
<strong><i>Reference #3</i></strong>
</p>

---

<p align="center">
  <img src="https://github.com/user-attachments/assets/8fcabfc4-9df8-4df5-bb6a-541bf4610a88">
</p>

<p align="center">
<strong><i>Reference #4 (pg.12) </i></strong>
</p>

Alright, I know that is a lot to take in but let's break it down. Remember how I said the "main" point of PCA was dimensionality reduction; the main way to achieve that is through the two methods (maximizing variance and minimizing residuals). However, before I talk about what those two things even mean, let's go back to the second picture. 


  
<p align="center">

$$
\begin{align}
\large{x_i:} &\text{Think of this as some data point vector} \\
\\
\large{\lvert\lvert x_i \rvert \rvert} \longrightarrow \large{\sqrt{x^2_1 + x^2_2 + ...}} &\text{Basically, square the coordinates of the data points and add them all up; then take the square root at the end}  \\
\\
\large{\vec{v_1}:} &\text{A unit vector onto which $\large{x_i}$ will be projected on} \\
\large{x'_i:} &\text{The projection of the datapoint onto the $\vec{v_1}$ unit vector which is a coordinate and not a scalar} \\
\large{x_i - x'_i = e_i:} &\text{Think of this as like the residuals} \\
\large{a_i{_1} = \langle x_i, v_1 \rangle} &\text{The "length" of the projection of the point onto the $\vec{v_1}$ unit vector which is a scalar}
\end{align}
$$
</p>

Okay, hopefully the second picture should be more clear now that all the symbols and such were explained. The reason for the subscript of i, is that we will have more than one data point. It could be $x_1, x_2, x_3, ...$ Now, let's move on to what it means to project a data point to some unit vector. Remember, a **unit vector** is a vector in which the components like $\langle x_1, x_2 \rangle$ add up to 1 after squaring each component. Let's take a look at the picture below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/fa07da87-88ae-4f02-8167-f4e49abb6f8b">
</p>

When we project that $\vec{a}$ onto $\vec{b}$ , it's like we go straight down from where $\vec{a}$ is pointing to and end up at $\vec{b}$. There is a lot more math invovled when it comes to proving the equations and such but let's focus on \frac{a}{b} 



### The Nitty Gritty




### Bringing it all Together









### References 
(Warning: Clicking the links will auto-download a pdf!)

1. Lagrange Method:
> https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-20-PCA-annotated.pdf

2. Proofs:
> https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf

3. Visualizing PCA
> https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

4. Visualizing PCA/Proof
> https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-16-PCA.pdf

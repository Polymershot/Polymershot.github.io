## Understanding Principal Component Analysis (The Big Picture & Proofs)

**Prerequisites**:
- Linear Algebra
- Calculus 3
- Up to Junior level Stat Courses (Probability/Stats Theory and Such)

This is mainly meant to strengthen the conceptial understanding of PCA (with some proofs that aren't too scary). If this post isn't too in-depth for you, feel free to just check out the references. I mainly want this guide to help other stats students like me who struggle. 

---

### The Setup

##### ***Dimensionality Reduction***

When you first think of PCA, you probably think about dimensionality reduction right? We want to reduce the number of columns/dimensions while preserving as much information as possible. 
For example, it's like turning your data that resembles a sphere into something elliptical.

<p align="center">

  <img src="https://github.com/user-attachments/assets/1fb2e58a-c830-47ae-b7a4-65fec60afac0">

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/6aa956aa-4321-4889-8196-a2b499b63f7a">

</p>

#### ***Data Transformation(s)***
Now that we've estatblished what the general idea of dimensionality reduction, let's discuss what needs to be done with the data before we procced with PCA. The main thing to remember is that the data should be centered and scaled for "computation" purposes. Centering the data means each column in your data should have a **mean of 0** and scaling the data means that each column in the data should be in terms of the **same scale (units)**. The mean being set to 0 should make sense later on hopefully and the scaling is due to the fact that some columns might be "favored" more due to their units. To apply this kind of transofrmation, **z-score standardization** is used. From this, each column will have $$\mu$$ = 0 and $$\sigma$$ = 1.

<p align="center">

  <img src="https://github.com/user-attachments/assets/90b37e0e-9de3-4242-9174-e114432bbed0">
  <strong><i>Reference #4 (pg.4)</i></strong> 

</p>

<p align="center">

  <img src="https://github.com/user-attachments/assets/822e00c0-2ea5-4b97-a698-91a0495fcd86">


</p>

If this doesn't make sense or if you want to go more in-depth, please refer to Reference #4 (pg.6).


### Visualizing the Problem

Let's focus on the two-dimensional case so we can understand what the goal of PCA is. Take a look at the picture below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/8fcabfc4-9df8-4df5-bb6a-541bf4610a88">
</p>

<p align="center">
<strong><i>Reference #4 (pg.12) </i></strong>
</p>

Alright, I know that is a lot to take in but let's break it down. Remember how I said the "main" point of PCA was dimensionality reduction; the main way to achieve that is through the two methods (maximizing variance and minimizing residuals). However, before I talk about what those two things even mean, let's go back to the second picture. 


  
<p align="center">

$$
\begin{align}
\large{x_i:} &\text{Think of this as some data point vector} \\
\\
\large{\lVert x_i  \rVert} \longrightarrow \large{\sqrt{x^2_1 + x^2_2 + ...}} &\text{Basically, square the coordinates of the data points and add them all up; then take the square root at the end}  \\
\\
\large{\vec{v_1}:} &\text{A unit vector onto which $\large{x_i}$ will be projected on} \\
\large{x'_i = \langle x_i, v_1 \rangle v_1 :} &\text{The projection of the data point (vector) onto the $\vec{v_1}$ unit vector which is a coordinate and not a scalar} \\
\large{x_i - x'_i = e_i:} &\text{Think of this as like the residuals} \\
\large{a_i{_1} = \langle x_i, v_1 \rangle} &\text{The "length" of the projection of the data point (vector) onto the $\vec{v_1}$ unit vector which is a scalar}
\end{align}
$$
</p>

Okay, hopefully the second picture should be more clear now that all the symbols and such were explained. The reason for the subscript of i, is that we will have more than one data point. It could be $x_1, x_2, x_3, ...$ Now, let's move on to what it means to project a data point to some unit vector. Remember, a **unit vector** is a vector in which the components like $\langle x_1, x_2 \rangle$ add up to 1 after squaring each component. The length of a vector is shown/computed as a dot product ($a \cdot b$) or by the **norm** ($\lVert x \rVert$). Let's take a look at the picture below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/fa07da87-88ae-4f02-8167-f4e49abb6f8b">
</p>

When we project that $\vec{a}$ onto $\vec{b}$ , it's like we go straight down from where $\vec{a}$ is pointing to and end up at $\vec{b}$. There is a lot more math invovled when it comes to proving the equations and such but let's focus on $\large{{a \cdot b \over \lVert b \rVert }}$. For our particular scenario, we know that $\large{\lVert v_1  \rVert}$ will be one since it is a unit vector. Therefore, in order to project our data point onto $v_1$, all we need to do is $\langle x_i, v_1 \rangle$ or $x_i \cdot v_1$ which is known as the **dot product**. Now, after we perform this kind of dot product, we would get some scalar/constant which represents the length/magnitude of the projection of $\vec{x_i}$ onto $\vec{v_1}$. To get the actual vector coordinates, we would do the following:  $\langle x_i, v_1 \rangle v_1 $. After we do that dot product, we multiply that constant times $\vec{v_1}$. What exactly is that $\vec{v_1}$ though? Well, that brings us back to our original problem and what it means to maximize variance and minimize residuals. Let's go back to our example in the 2d dimension. We want to find some $\vec{v_1}$ vector to project our data onto. However, we want some sort of objective so we can narrow down our options. Take a look at the picture below.

<p align="center">
  <img src="https://github.com/user-attachments/assets/946841ff-b164-449e-8d70-e4ac6a9a9b6d">
</p>

<p align="center">
<strong><i>Reference #3</i></strong>
</p>

The first picture in this section shows how one data vector is projected onto $\vec{v_1}$ but now this picture shows how it looks like when you apply the same concepts to all the data. Basically, we want to find the best $\vec{v_1}$ according to our two "objectives".
<br/>


<p align="center">
<b>Maximizing Variance</b>: When we project the data onto $\vec{v_1}$, we want those data points to be spread out in terms of distance from the origin. 
<br/>
<b>Minimizing Residuals</b>: When we project the data onto $\vec{v_1}$, we also want to minimize that distance from the original data point and the new, projected data point
</p>

On the next section, we will see these two are actually equivalent by using other proofs/references. 

**Note**: I am being very loose with how I'm saying data point/vector but from what I understand, those coordinates/points are expressed in vector format when performing the kind of operations mentioned above. Please feel free to comment any errors with this or in general.

### The Nitty Gritty

##### ***Minimizing Residuals***

**Note**: The following proof is from Reference #2 but they made some silly errors so I have to redo them. They switched the x and w vectors. This sort of dot product is not possible because of dimensional issues. It would be a px1 vector times a 1xp matrix which is not possible for doing dot products since the inner dimensions must match as far as I'm aware.

Alright, if you aren't familiar with Mean Squared Error (MSE) or just forgot it, just do a quick google search. Basically, MSE is a way to quantify error, which is mainly used for linear regression. However, in our case, we have residuals due to the data point and its projection onto $\vec{v_1}$. To keep notation consistent with the proofs, let's call $\vec{v_1}$  $\longrightarrow$ $\vec{w}$ now. What we are trying to do is first see how to calculate a "residual error" in terms of the data point/vector and its projection onto $\vec{w}$.

<p align="center">
$$
\begin{align}
\large{ {\lVert {\vec{x_i} - (\vec{x_i} \cdot \vec{w})} \vec{w} \rVert}^2 } &=& \large{ (\vec{x_i} - (\vec{x_i} \cdot \vec{w}) \vec{w}  ) \cdot (\vec{x_i} - (\vec{x_i} \cdot \vec{w}) \vec{w})} &&\text{Squared distance from the data point to its projection onto $\vec{w}$} \\
\large{ \vec{x_i} \cdot \vec{x_i} - \vec{x_i} \cdot (\vec{x_i} \cdot \vec{w}) \vec{w} \\ -  (\vec{x_i} \cdot \vec{w}) \vec{w} \cdot \vec{x_i} + (\vec{x_i} \cdot \vec{w}) \vec{w} \cdot (\vec{x_i} \cdot \vec{w}) \vec{w} } \\
\large{MSE(\vec{w})} &=& \large{\frac{1}{n} \sum_{i=1}^n {\lVert \vec{x_i} \rVert}^2 - (\vec{x_i} \cdot \vec{w})^2} \\
&=& \large{\frac{1}{n} (\sum_{i=1}^n {\lVert \vec{x_i} \rVert}^2 - (\vec{x_i} \cdot \vec{w})^2)} \\
\end{align}
$$
</p>




### Bringing it all Together









### References 
(Warning: Clicking the links will auto-download a pdf!)

1. Lagrange Method:
> https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-20-PCA-annotated.pdf

2. Proofs:
> https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf

3. Visualizing PCA
> https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

4. Visualizing PCA/Proof
> https://web.lums.edu.pk/~imdad/pdfs/CS5312_Notes/CS5312_Notes-16-PCA.pdf
